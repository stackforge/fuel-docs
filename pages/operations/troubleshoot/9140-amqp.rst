.. index:: HowTo: Troubleshoot AMQP issues

.. _tshoot-amqp-ops:

How To Troubleshoot AMQP issues
===============================

AMQP is the heart of OpenStack. If something gets wrong with
the messaging layer, normally an OpenStack application can tolerate
this by issuing reconnects, reporting some requests as failed or
retrying them. While this depends on the particular application
and the underlying Oslo messaging library design, there are also some
generic health checks and troubleshooting steps for operators to
know and to do.

Check if there is a problem in the Corosync/Pacemaker layer
-----------------------------------------------------------

Normally, failures of the RabbitMQ multistate resource are automatically
fixed by Corosync and Pacemaker. But if a failure cannot be automatically fixed
for some reason, the master_p_rabbitmq-server resource status will be considered "bad".
The status is considered "good" when there exists a single master for the
master_p_rabbitmq-server and the rest of the resource instances are reported
as slaves. Note, that the command ``pcs status`` issued on all controllers
should be enough to gather all the required information, see :ref:`crm-ops`.
You can also try issuing an extended output with the crm tool alternative:

::

    crm_mon -fotAW -1

If there are some RabbitMQ resource failures, they will
be shown in the command output with the time stamps, so you can
search for the events in the logs around that moment.

.. note:: If there are split clusters of Corosync running, you should
  first fix your Corosync cluster, because you cannot resolve issues with the
  Pacemaker resources, including RabbitMQ cluster, when there is a
  split brain in the Corosync cluster.

How to recover
++++++++++++++

It is recommended to clean up and restart the master_p_rabbitmq-server
Pacemaker resource, see :ref:`crm-ops`.

.. note:: Restarting the RabbitMQ Pacemaker resource will introduce
  a full downtime for the AMQP cluster and OpenStack applications.
  The downtime may take from a few to up to 20 minutes.

Check if there is a problem in the RabbitMQ layer
-------------------------------------------------

Normally, failures of the RabbitMQ cluster are automatically healed
by OCF resource agents in Pacemaker. But if this fails for some
reason, there may be unsynchronized queues, wrong cluster membership
reported, or partitions detected by the rabbitmqctl tool, or even
some list channels/queues requests may hang. Note, that the
command ``rabbitmqctl report`` issued on all controllers should be enough
to gather all the required information, but there is also a special group
of Fuel OSTF HA health checks available in the Fuel UI and CLI. See also
`RabbitMQ OSTF replication tests <https://blueprints.launchpad.net/fuel/+spec/ostf-rabbit-replication-tests>`.

How to recover
++++++++++++++

It is recommended to clean up and restart the master_p_rabbitmq-server
Pacemaker resource, see :ref:`crm-ops`.

Check if there is a problem in the Oslo messaging layer
-------------------------------------------------------

Note, that normally an OpenStack application should be able to
reconnect the AMQP host and restore its operations, eventually.
But if it cannot for some reason, there may be "down" status reports
or failures of CLI commands and messaging related or
publish/consume message related records in the log files of the OpenStack
services relying on the Oslo messaging library.
For example, there may be records in the log files
similar to the following ones:

::

    Timed out waiting for a reply to message...

How to recover
++++++++++++++

It is recommended to restart the affected OpenStack service or
services, see :ref:`manage-openstack-services-op`.

.. note:: Restarting the OpenStack service will introduce
  a short (near to zero) downtime for the related OpenStack application.

Check if there are AMQP problems with any of the OpenStack components
---------------------------------------------------------------------

Note, that normally an OpenStack application should be able to
reconnect the AMQP host and restore its operations, eventually.
But if it cannot for some reason, there may be "down" status reports
or failures of CLI commands and AMQP/messaging related records in the log
files of the services belonging to the affected OpenStack component
under verification.
For Nova, for example, there may be records in the log files in
the /var/log/nova/ directory similar to the following ones:

::

    AMQP server on ... is unreachable: [Errno 113] EHOSTUNREACH...

How to recover
++++++++++++++

It is recommended to restart all instances of the OpenStack services
related to the affected OpenStack component,
see :ref:`manage-openstack-services-op`.
For example, for Nova Compute component, you may want to restart all
instances of the Nova services on the Controllers and Compute nodes affected
by the AMQP issue.
